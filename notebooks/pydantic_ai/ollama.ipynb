{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a23583b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Imports completed successfully!\n",
      "ðŸŒ Ollama endpoint: http://localhost:11434\n",
      "ðŸ¤– Model: deepseek-r1-qwen3-8b:latest\n",
      "ðŸ’¡ Using OpenAI-compatible interface with Ollama\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Optional, Any\n",
    "from pydantic import BaseModel, Field\n",
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.models.openai import OpenAIModel\n",
    "from pydantic_ai.providers.openai import OpenAIProvider\n",
    "\n",
    "# Configuration for Ollama using OpenAI-compatible endpoint\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "MODEL_NAME = \"deepseek-r1-qwen3-8b:latest\"  # Updated to use available model\n",
    "\n",
    "# Ollama provides OpenAI-compatible API at /v1/ endpoint\n",
    "ollama_model = OpenAIModel(\n",
    "    model_name=MODEL_NAME, provider=OpenAIProvider(base_url=f\"{OLLAMA_BASE_URL}/v1\")\n",
    ")\n",
    "\n",
    "print(\"âœ… Imports completed successfully!\")\n",
    "print(f\"ðŸŒ Ollama endpoint: {OLLAMA_BASE_URL}\")\n",
    "print(f\"ðŸ¤– Model: {MODEL_NAME}\")\n",
    "print(\"ðŸ’¡ Using OpenAI-compatible interface with Ollama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c19ea82e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModelHTTPError",
     "evalue": "status_code: 400, model_name: deepseek-r1-qwen3-8b:latest, body: {'message': 'registry.ollama.ai/library/deepseek-r1-qwen3-8b:latest does not support tools', 'type': 'api_error', 'param': None, 'code': None}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ai_stack/notebooks/pydantic_ai/.venv/lib/python3.12/site-packages/pydantic_ai/models/openai.py:293\u001b[39m, in \u001b[36mOpenAIModel._completions_create\u001b[39m\u001b[34m(self, messages, stream, model_settings, model_request_parameters)\u001b[39m\n\u001b[32m    292\u001b[39m     extra_headers.setdefault(\u001b[33m'\u001b[39m\u001b[33mUser-Agent\u001b[39m\u001b[33m'\u001b[39m, get_user_agent())\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.client.chat.completions.create(\n\u001b[32m    294\u001b[39m         model=\u001b[38;5;28mself\u001b[39m._model_name,\n\u001b[32m    295\u001b[39m         messages=openai_messages,\n\u001b[32m    296\u001b[39m         parallel_tool_calls=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    297\u001b[39m         tools=tools \u001b[38;5;129;01mor\u001b[39;00m NOT_GIVEN,\n\u001b[32m    298\u001b[39m         tool_choice=tool_choice \u001b[38;5;129;01mor\u001b[39;00m NOT_GIVEN,\n\u001b[32m    299\u001b[39m         stream=stream,\n\u001b[32m    300\u001b[39m         stream_options={\u001b[33m'\u001b[39m\u001b[33minclude_usage\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m} \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m NOT_GIVEN,\n\u001b[32m    301\u001b[39m         stop=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mstop_sequences\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    302\u001b[39m         max_completion_tokens=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    303\u001b[39m         timeout=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    304\u001b[39m         seed=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    305\u001b[39m         reasoning_effort=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mopenai_reasoning_effort\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    306\u001b[39m         user=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mopenai_user\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    307\u001b[39m         service_tier=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mopenai_service_tier\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    308\u001b[39m         temperature=sampling_settings.get(\u001b[33m'\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    309\u001b[39m         top_p=sampling_settings.get(\u001b[33m'\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    310\u001b[39m         presence_penalty=sampling_settings.get(\u001b[33m'\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    311\u001b[39m         frequency_penalty=sampling_settings.get(\u001b[33m'\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    312\u001b[39m         logit_bias=sampling_settings.get(\u001b[33m'\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    313\u001b[39m         logprobs=sampling_settings.get(\u001b[33m'\u001b[39m\u001b[33mopenai_logprobs\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    314\u001b[39m         top_logprobs=sampling_settings.get(\u001b[33m'\u001b[39m\u001b[33mopenai_top_logprobs\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    315\u001b[39m         extra_headers=extra_headers,\n\u001b[32m    316\u001b[39m         extra_body=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mextra_body\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m    317\u001b[39m     )\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m APIStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ai_stack/notebooks/pydantic_ai/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:2028\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   2027\u001b[39m validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m2028\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2029\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2030\u001b[39m     body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2031\u001b[39m         {\n\u001b[32m   2032\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   2033\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2034\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   2035\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   2036\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   2037\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   2038\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   2039\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   2040\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   2041\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   2042\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2043\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   2044\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   2045\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2046\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   2047\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   2048\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   2049\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   2050\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   2051\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2052\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   2053\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2054\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2055\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2056\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2057\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2058\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2059\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2060\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2061\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2062\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   2063\u001b[39m         },\n\u001b[32m   2064\u001b[39m         completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m   2065\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2066\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m   2067\u001b[39m     ),\n\u001b[32m   2068\u001b[39m     options=make_request_options(\n\u001b[32m   2069\u001b[39m         extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2070\u001b[39m     ),\n\u001b[32m   2071\u001b[39m     cast_to=ChatCompletion,\n\u001b[32m   2072\u001b[39m     stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2073\u001b[39m     stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   2074\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ai_stack/notebooks/pydantic_ai/.venv/lib/python3.12/site-packages/openai/_base_client.py:1748\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1745\u001b[39m opts = FinalRequestOptions.construct(\n\u001b[32m   1746\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1747\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1748\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ai_stack/notebooks/pydantic_ai/.venv/lib/python3.12/site-packages/openai/_base_client.py:1555\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1554\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1555\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1557\u001b[39m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': 'registry.ollama.ai/library/deepseek-r1-qwen3-8b:latest does not support tools', 'type': 'api_error', 'param': None, 'code': None}}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mModelHTTPError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      6\u001b[39m ollama_model = OpenAIModel(\n\u001b[32m      7\u001b[39m     model_name=MODEL_NAME, provider=OpenAIProvider(base_url=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOLLAMA_BASE_URL\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/v1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m )\n\u001b[32m      9\u001b[39m agent = Agent(ollama_model, output_type=CityLocation)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m agent.run(\u001b[33m'\u001b[39m\u001b[33mWhere were the olympics held in 2012?\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(result.output)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m#> city='London' country='United Kingdom'\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ai_stack/notebooks/pydantic_ai/.venv/lib/python3.12/site-packages/pydantic_ai/agent.py:468\u001b[39m, in \u001b[36mAgent.run\u001b[39m\u001b[34m(self, user_prompt, output_type, message_history, model, deps, model_settings, usage_limits, usage, infer_name, **_deprecated_kwargs)\u001b[39m\n\u001b[32m    456\u001b[39m     output_type = _deprecated_kwargs[\u001b[33m'\u001b[39m\u001b[33mresult_type\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    458\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter(\n\u001b[32m    459\u001b[39m     user_prompt=user_prompt,\n\u001b[32m    460\u001b[39m     output_type=output_type,\n\u001b[32m   (...)\u001b[39m\u001b[32m    466\u001b[39m     usage=usage,\n\u001b[32m    467\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m agent_run:\n\u001b[32m--> \u001b[39m\u001b[32m468\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m agent_run:\n\u001b[32m    469\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    471\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m agent_run.result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m'\u001b[39m\u001b[33mThe graph run did not finish properly\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ai_stack/notebooks/pydantic_ai/.venv/lib/python3.12/site-packages/pydantic_ai/agent.py:1922\u001b[39m, in \u001b[36mAgentRun.__anext__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1918\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__anext__\u001b[39m(\n\u001b[32m   1919\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1920\u001b[39m ) -> _agent_graph.AgentNode[AgentDepsT, OutputDataT] | End[FinalResult[OutputDataT]]:\n\u001b[32m   1921\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Advance to the next node automatically based on the last returned node.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1922\u001b[39m     next_node = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._graph_run.\u001b[34m__anext__\u001b[39m()\n\u001b[32m   1923\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _agent_graph.is_agent_node(next_node):\n\u001b[32m   1924\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m next_node\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ai_stack/notebooks/pydantic_ai/.venv/lib/python3.12/site-packages/pydantic_graph/graph.py:809\u001b[39m, in \u001b[36mGraphRun.__anext__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    806\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m._next_node, End):\n\u001b[32m    807\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m809\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.next(\u001b[38;5;28mself\u001b[39m._next_node)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ai_stack/notebooks/pydantic_ai/.venv/lib/python3.12/site-packages/pydantic_graph/graph.py:782\u001b[39m, in \u001b[36mGraphRun.next\u001b[39m\u001b[34m(self, node)\u001b[39m\n\u001b[32m    780\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.persistence.record_run(node_snapshot_id):\n\u001b[32m    781\u001b[39m         ctx = GraphRunContext(\u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.deps)\n\u001b[32m--> \u001b[39m\u001b[32m782\u001b[39m         \u001b[38;5;28mself\u001b[39m._next_node = \u001b[38;5;28;01mawait\u001b[39;00m node.run(ctx)\n\u001b[32m    784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m._next_node, End):\n\u001b[32m    785\u001b[39m     \u001b[38;5;28mself\u001b[39m._snapshot_id = \u001b[38;5;28mself\u001b[39m._next_node.get_snapshot_id()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ai_stack/notebooks/pydantic_ai/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py:300\u001b[39m, in \u001b[36mModelRequestNode.run\u001b[39m\u001b[34m(self, ctx)\u001b[39m\n\u001b[32m    295\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._did_stream:\n\u001b[32m    296\u001b[39m     \u001b[38;5;66;03m# `self._result` gets set when exiting the `stream` contextmanager, so hitting this\u001b[39;00m\n\u001b[32m    297\u001b[39m     \u001b[38;5;66;03m# means that the stream was started but not finished before `run()` was called\u001b[39;00m\n\u001b[32m    298\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.AgentRunError(\u001b[33m'\u001b[39m\u001b[33mYou must finish streaming before calling run()\u001b[39m\u001b[33m'\u001b[39m)  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_request(ctx)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ai_stack/notebooks/pydantic_ai/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py:353\u001b[39m, in \u001b[36mModelRequestNode._make_request\u001b[39m\u001b[34m(self, ctx)\u001b[39m\n\u001b[32m    351\u001b[39m model_settings, model_request_parameters = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._prepare_request(ctx)\n\u001b[32m    352\u001b[39m model_request_parameters = ctx.deps.model.customize_request_parameters(model_request_parameters)\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m model_response = \u001b[38;5;28;01mawait\u001b[39;00m ctx.deps.model.request(\n\u001b[32m    354\u001b[39m     ctx.state.message_history, model_settings, model_request_parameters\n\u001b[32m    355\u001b[39m )\n\u001b[32m    356\u001b[39m ctx.state.usage.incr(_usage.Usage())\n\u001b[32m    358\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._finish_handling(ctx, model_response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ai_stack/notebooks/pydantic_ai/.venv/lib/python3.12/site-packages/pydantic_ai/models/openai.py:216\u001b[39m, in \u001b[36mOpenAIModel.request\u001b[39m\u001b[34m(self, messages, model_settings, model_request_parameters)\u001b[39m\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(\n\u001b[32m    210\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    211\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[ModelMessage],\n\u001b[32m    212\u001b[39m     model_settings: ModelSettings | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    213\u001b[39m     model_request_parameters: ModelRequestParameters,\n\u001b[32m    214\u001b[39m ) -> ModelResponse:\n\u001b[32m    215\u001b[39m     check_allow_model_requests()\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._completions_create(\n\u001b[32m    217\u001b[39m         messages, \u001b[38;5;28;01mFalse\u001b[39;00m, cast(OpenAIModelSettings, model_settings \u001b[38;5;129;01mor\u001b[39;00m {}), model_request_parameters\n\u001b[32m    218\u001b[39m     )\n\u001b[32m    219\u001b[39m     model_response = \u001b[38;5;28mself\u001b[39m._process_response(response)\n\u001b[32m    220\u001b[39m     model_response.usage.requests = \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ai_stack/notebooks/pydantic_ai/.venv/lib/python3.12/site-packages/pydantic_ai/models/openai.py:320\u001b[39m, in \u001b[36mOpenAIModel._completions_create\u001b[39m\u001b[34m(self, messages, stream, model_settings, model_request_parameters)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m APIStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    319\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (status_code := e.status_code) >= \u001b[32m400\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ModelHTTPError(status_code=status_code, model_name=\u001b[38;5;28mself\u001b[39m.model_name, body=e.body) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    321\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mModelHTTPError\u001b[39m: status_code: 400, model_name: deepseek-r1-qwen3-8b:latest, body: {'message': 'registry.ollama.ai/library/deepseek-r1-qwen3-8b:latest does not support tools', 'type': 'api_error', 'param': None, 'code': None}"
     ]
    }
   ],
   "source": [
    "class CityLocation(BaseModel):\n",
    "    city: str\n",
    "    country: str\n",
    "\n",
    "\n",
    "ollama_model = OpenAIModel(\n",
    "    model_name=MODEL_NAME, provider=OpenAIProvider(base_url=f\"{OLLAMA_BASE_URL}/v1\")\n",
    ")\n",
    "agent = Agent(ollama_model, output_type=CityLocation)\n",
    "\n",
    "result = await agent.run('Where were the olympics held in 2012?')\n",
    "print(result.output)\n",
    "#> city='London' country='United Kingdom'\n",
    "print(result.usage())\n",
    "#> Usage(requests=1, request_tokens=57, response_tokens=8, total_tokens=65)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
